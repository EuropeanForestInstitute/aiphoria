{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# %%\n",
    "# *****************************************************\n",
    "# * Import required packages and set up path for ODYM *\n",
    "# *****************************************************\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from core.dataprovider import DataProvider\n",
    "from core.datachecker import DataChecker\n",
    "from core.datastructures import Scenario\n",
    "from core.datavisualizer import DataVisualizer\n",
    "from core.network_graph import NetworkGraph\n",
    "from core.flowsolver import FlowSolver\n",
    "from core.parameters import ParameterName\n",
    "\n",
    "# Path configuration\n",
    "sys.path.insert(0, os.path.join('.'))\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '.', 'lib', 'odym', 'modules'))\n",
    "\n",
    "# ODYM classes\n",
    "import ODYM_Classes as msc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T09:39:30.444279300Z",
     "start_time": "2024-12-19T09:39:29.063199400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file data/example_data.xlsx...\n",
      "Using following parameters for running the model:\n",
      "\tsheet_name_processes            = Processes\n",
      "\tcolumn_range_processes          = B:R\n",
      "\tskip_num_rows_processes         = 2\n",
      "\tsheet_name_flows                = Flows\n",
      "\tcolumn_range_flows              = B:U\n",
      "\tskip_num_rows_flows             = 2\n",
      "\tstart_year                      = 2021\n",
      "\tend_year                        = 2030\n",
      "\tdetect_year_range               = True\n",
      "\tuse_virtual_flows               = True\n",
      "\tvirtual_flows_epsilon           = 0.1\n",
      "\tconversion_factor_c_to_co2      = 3.67\n",
      "\tfill_missing_absolute_flows     = True\n",
      "\tfill_missing_relative_flows     = True\n",
      "\tfill_method                     = Previous\n",
      "\tsheet_name_scenarios            = Scenarios\n",
      "\tcreate_network_graphs           = False\n",
      "\tcreate_sankey_charts            = True\n"
     ]
    }
   ],
   "source": [
    "# ******************************************************\n",
    "# * Step 1: Load the data from Excel file DataProvider *\n",
    "# ******************************************************\n",
    "\n",
    "# Load data from file using the DataProvider\n",
    "# and get model parameters from the file\n",
    "filename = \"data/example_data.xlsx\"\n",
    "\n",
    "print(\"Loading data from file {}...\".format(filename))\n",
    "dataprovider = DataProvider(filename)\n",
    "\n",
    "# Model parameters is a Dictionary that contains loaded data from Excel sheet named \"Settings\"\n",
    "# and are used for running the FlowSolver and setting up ODYM\n",
    "model_params = dataprovider.get_model_params()\n",
    "print(\"Using following parameters for running the model:\")\n",
    "for param_name, param_value in model_params.items():\n",
    "    print(\"\\t{:32}= {}\".format(param_name, param_value))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T09:39:31.113380Z",
     "start_time": "2024-12-19T09:39:31.031221300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking errors in data...\n",
      "Checking stock distribution types...\n",
      "Checking stock distribution parameters...\n",
      "Checking process total inflows and total outflows mismatches...\n",
      "Checking flow type changes...\n",
      "Checking for processes that have no inflows and only relative outflows...\n",
      "Checking scenario definitions...\n",
      "Building baseline scenario...\n",
      "Building 0 alternative scenarios...\n"
     ]
    }
   ],
   "source": [
    "# **************************************************************\n",
    "# * Step 2: Check data integrity and build data for FlowSolver *\n",
    "# **************************************************************\n",
    "\n",
    "print(\"Checking errors in data...\")\n",
    "data_checker = DataChecker(dataprovider)\n",
    "scenarios = data_checker.build_scenarios()\n",
    "is_checker_ok, checker_messages = data_checker.check_for_errors()\n",
    "if not is_checker_ok:\n",
    "    for msg in checker_messages:\n",
    "        print(msg)\n",
    "    SystemExit(-1)\n",
    "\n",
    "\n",
    "# Create network graph for data\n",
    "# scenarios[0] is always the baseline scenario and is guaranteed to exist\n",
    "if model_params[ParameterName.CreateNetworkGraphs]:\n",
    "    network_visualizer = NetworkGraph()\n",
    "    network_visualizer.build(scenarios[0].scenario_data)\n",
    "    network_visualizer.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T09:39:31.703252Z",
     "start_time": "2024-12-19T09:39:31.684572700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving flows for year 2021/2021: : 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered processes that could not be evaluated in year 2021:\n",
      "The following processes have no inflows and have ONLY relative outflows (= error in data)\n",
      "Possible ways to to fix:\n",
      "- Introducing a valid inflow to the process\n",
      "- Ensure that a valid inflow is present for the process in the model's initial year\n",
      "\n",
      "List of invalid process IDs:\n",
      "\n",
      "List of unevaluated flows:\n",
      "\tFlow 'Construction:FI' -> 'Sawmilling:FI': Value=40.0 Unit=%,is_evaluated=False, evaluated_share=0.4, evaluated_value=0.0,year=2021, is_virtual=False\n",
      "\tFlow 'Sawmilling:FI' -> 'Sawnwood:FI': Value=100.0 Unit=%,is_evaluated=False, evaluated_share=1.0, evaluated_value=0.0,year=2021, is_virtual=False\n",
      "\tFlow 'Sawnwood:FI' -> 'Construction:FI': Value=60.0 Unit=%,is_evaluated=False, evaluated_share=0.6, evaluated_value=0.0,year=2021, is_virtual=False\n",
      "\tFlow 'Sawnwood:FI' -> 'Furniture:FI': Value=40.0 Unit=%,is_evaluated=False, evaluated_share=0.4, evaluated_value=0.0,year=2021, is_virtual=False\n",
      "\tFlow 'Construction:FI' -> 'Incineration:FI': Value=60.0 Unit=%,is_evaluated=False, evaluated_share=0.6, evaluated_value=0.0,year=2021, is_virtual=False\n",
      "\tFlow 'Furniture:FI' -> 'Incineration:FI': Value=100.0 Unit=%,is_evaluated=False, evaluated_share=1.0, evaluated_value=0.0,year=2021, is_virtual=False\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "-100",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\u001B[1;31m:\u001B[0m -100\n"
     ]
    }
   ],
   "source": [
    "# ******************************************************************\n",
    "# * Step 3: Solve flows for baseline scenario using the FlowSolver *\n",
    "# ******************************************************************\n",
    "for scenario_index, scenario in enumerate(scenarios):\n",
    "    # NOTE: Baseline scenario is always the first element in the list\n",
    "    # and all the alternative scenarios (if any) are after that\n",
    "    if scenario_index == 0:\n",
    "        # Process baseline scenario\n",
    "        baseline_flow_solver = FlowSolver(scenario=scenario)\n",
    "        baseline_flow_solver.solve_timesteps()\n",
    "        scenario.flow_solver = baseline_flow_solver\n",
    "    else:\n",
    "        # Get and copy solved scenario data from baseline scenario flow solver\n",
    "        baseline_scenario_data = scenarios[0].flow_solver.get_solved_scenario_data()\n",
    "        scenario.copy_from_baseline_scenario_data(baseline_scenario_data)\n",
    "\n",
    "        # Solve this alternative scenario time steps\n",
    "        scenario_flow_solver = FlowSolver(scenario=scenario)\n",
    "        scenario_flow_solver.solve_timesteps()\n",
    "        scenario.flow_solver = scenario_flow_solver"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T09:39:32.914402900Z",
     "start_time": "2024-12-19T09:39:32.391111500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving flows for year 2021/2021: : 0it [00:03, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ODYM MFA for scenario 'Baseline' (1/1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_unique_processes'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 46\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# Get inflow values to stock\u001B[39;00m\n\u001B[0;32m     45\u001B[0m year_index_to_year \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28menumerate\u001B[39m(model_years))\n\u001B[1;32m---> 46\u001B[0m unique_processes \u001B[38;5;241m=\u001B[39m \u001B[43mflow_solver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_unique_processes\u001B[49m()\n\u001B[0;32m     47\u001B[0m unique_flows \u001B[38;5;241m=\u001B[39m flow_solver\u001B[38;5;241m.\u001B[39mget_unique_flows()\n\u001B[0;32m     49\u001B[0m \u001B[38;5;66;03m# NOTE: These are not used anywhere\u001B[39;00m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# # DataFrames for Processes, Flows and Flow values\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# print(\"Collecting processes to DataFrame...\")\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# print(\"Collecting evaluated flow values to DataFrame...\")\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# df_flow_values = flow_solver.get_evaluated_flow_values_as_dataframe()\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'get_unique_processes'"
     ]
    }
   ],
   "source": [
    "# ************************************************************************\n",
    "# * Step 4: Setup ODYM classifications and index table for each Scenario *\n",
    "# ************************************************************************\n",
    "for scenario_index, scenario in enumerate(scenarios):\n",
    "    print(\"Building ODYM MFA for scenario '{}' ({}/{})...\".format(scenario.name, scenario_index + 1, len(scenarios)))\n",
    "    # NOTE: ODYM MFA is only used for mass balance check?\n",
    "\n",
    "    # Track solid wood equivalent and carbon.\n",
    "    # Dictionary of classifications enters the index table defined for the system.\n",
    "    # The index table lists all aspects needed and assigns a classification and index letter to each aspect.\n",
    "    scenario_data = scenario.scenario_data\n",
    "    years = scenario_data.years\n",
    "\n",
    "    model_time_start = scenario_data.start_year\n",
    "    model_time_end = scenario_data.end_year\n",
    "    model_elements = ['Solid wood equivalent', 'Carbon']\n",
    "    model_years = years\n",
    "\n",
    "    model_classifications = {\n",
    "        'Time': msc.Classification(Name='Time', Dimension='Time', ID=1, Items=model_years),\n",
    "        'Cohort': msc.Classification(Name='Age-cohort', Dimension='Time', ID=2, Items=model_years),\n",
    "        'Element': msc.Classification(Name='Elements', Dimension='Element', ID=3, Items=model_elements),\n",
    "    }\n",
    "\n",
    "    index_table = pd.DataFrame({'Aspect': ['Time', 'Age-cohort', 'Element'],  # 'Time' and 'Element' must be present!\n",
    "                                'Description': ['Model aspect \"time\"', 'Model aspect \"age-cohort\"', 'Model aspect \"Element\"'],\n",
    "                                'Dimension': ['Time', 'Time', 'Element'],  # 'Time' and 'Element' are also dimensions\n",
    "                                'Classification': [model_classifications[Aspect] for Aspect in ['Time', 'Cohort', 'Element']],\n",
    "                                'IndexLetter': ['t', 'c', 'e' ]})  # Unique one letter (upper or lower case) indices to be used later for calculations.\n",
    "\n",
    "    index_table.set_index('Aspect', inplace=True)  # Default indexing of IndexTable, other indices are produced on the fly\n",
    "    index_table\n",
    "\n",
    "    # **************************************\n",
    "    # * Step 5: Initialize ODYM MFA system *\n",
    "    # **************************************\n",
    "    flow_solver = scenario.flow_solver\n",
    "\n",
    "    mfa_system = msc.MFAsystem(Name='Wood product system', Geogr_Scope='Europe', Unit='Mm3',\n",
    "                               ProcessList=[], FlowDict={}, StockDict={}, ParameterDict={},\n",
    "                               Time_Start=model_time_start, Time_End=model_time_end, IndexTable=index_table,\n",
    "                               Elements=index_table.loc['Element'].Classification.Items)\n",
    "\n",
    "    # Get inflow values to stock\n",
    "    year_index_to_year = dict(enumerate(model_years))\n",
    "    unique_processes = flow_solver.get_unique_processes()\n",
    "    unique_flows = flow_solver.get_unique_flows()\n",
    "\n",
    "    # NOTE: These are not used anywhere\n",
    "    # # DataFrames for Processes, Flows and Flow values\n",
    "    # print(\"Collecting processes to DataFrame...\")\n",
    "    # df_processes = flow_solver.get_processes_as_dataframe()\n",
    "    # print(\"Collecting flows to DataFrame...\")\n",
    "    # df_flows = flow_solver.get_flows_as_dataframe()\n",
    "    # print(\"Collecting evaluated flow values to DataFrame...\")\n",
    "    # df_flow_values = flow_solver.get_evaluated_flow_values_as_dataframe()\n",
    "\n",
    "    print(\"Creating ODYM objects...\")\n",
    "    # Create ODYM objects\n",
    "\n",
    "    print(\"Building ODYM processes...\")\n",
    "    odym_processes = []\n",
    "    process_id_to_index = {}\n",
    "    for process_id, process in unique_processes.items():\n",
    "        process_index = len(odym_processes)\n",
    "        process_id_to_index[process_id] = process_index\n",
    "        new_process = msc.Process(ID=process_index, Name=process.name)\n",
    "        odym_processes.append(new_process)\n",
    "\n",
    "    print(\"Building ODYM flows...\")\n",
    "    odym_flows = {}\n",
    "    for flow_id, flow in unique_flows.items():\n",
    "        source_process_index = process_id_to_index[flow.source_process_id]\n",
    "        target_process_index = process_id_to_index[flow.target_process_id]\n",
    "        new_flow = msc.Flow(ID=flow.id, P_Start=source_process_index, P_End=target_process_index, Indices='t,e', Values=None)\n",
    "        odym_flows[flow.id] = new_flow\n",
    "\n",
    "    print(\"Building ODYM stocks...\")\n",
    "    odym_stocks = {}\n",
    "    for stock in flow_solver.get_all_stocks():\n",
    "        process_index = process_id_to_index[stock.id]\n",
    "        new_stock = msc.Stock(ID=stock.id, Name=stock.name, P_Res=process_index, Indices='t,e', Type=0, Values=None)\n",
    "        odym_stocks[stock.id] = new_stock\n",
    "\n",
    "    mfa_system.ProcessList = odym_processes\n",
    "    mfa_system.FlowDict = odym_flows\n",
    "    mfa_system.StockDict = odym_stocks\n",
    "    mfa_system.Initialize_FlowValues()\n",
    "    mfa_system.Initialize_StockValues()\n",
    "    mfa_system.Consistency_Check()\n",
    "\n",
    "    # Update ODYM flow values from flow values DataFrame\n",
    "    for flow_id, flow in mfa_system.FlowDict.items():\n",
    "        for year_index, value in enumerate(flow.Values):\n",
    "            # Skip to next year if FlowSolver does not have data for this year\n",
    "            # This is possible because ODYM flow and stock values are already initialized to 0.0\n",
    "            flow_has_data_for_year = flow_solver.has_flow(year=year_index_to_year[year_index], flow_id=flow_id)\n",
    "            if not flow_has_data_for_year:\n",
    "                continue\n",
    "\n",
    "            # NOTE: Virtual flows use default value defined in Flow for carbon content (now 1.0).\n",
    "            solved_flow = flow_solver.get_flow(year=year_index_to_year[year_index], flow_id=flow_id)\n",
    "            flow.Values[year_index, 0] = solved_flow.evaluated_value\n",
    "            flow.Values[year_index, 1] = solved_flow.evaluated_value_carbon\n",
    "\n",
    "    # Process stocks (fill with data)\n",
    "    for stock_id, stock in odym_stocks.items():\n",
    "        # Calculate cohorts for \"Solid wood equivalent\"\n",
    "        dsm_swe = flow_solver.get_dynamic_stocks_swe()[stock_id]\n",
    "        swe_stock_by_cohort = dsm_swe.compute_s_c_inflow_driven()\n",
    "        swe_outflow_by_cohort = dsm_swe.compute_o_c_from_s_c()\n",
    "        swe_stock_total = dsm_swe.compute_stock_total()\n",
    "        swe_stock_change = dsm_swe.compute_stock_change()\n",
    "        stock.Values[:, 0] = swe_stock_change\n",
    "\n",
    "        # Calculate cohorts for \"Carbon\"\n",
    "        dsm_carbon = flow_solver.get_dynamic_stocks_carbon()[stock_id]\n",
    "        carbon_stock_by_cohort = dsm_carbon.compute_s_c_inflow_driven()\n",
    "        carbon_outflow_by_cohort = dsm_carbon.compute_o_c_from_s_c()\n",
    "        carbon_stock_total = dsm_carbon.compute_stock_total()\n",
    "        carbon_stock_change = dsm_carbon.compute_stock_change()\n",
    "        stock.Values[:, 1] = carbon_stock_change\n",
    "\n",
    "    print(\"Mass balance difference per year\")\n",
    "    mb = mfa_system.MassBalance()\n",
    "    print(\"Mass balance result shape: {}\".format(mb.shape))\n",
    "    df_mass_balance = pd.DataFrame(columns=[\"Year\", \"Process 0\", \"Rest\", \"Abs difference\"])\n",
    "    for year_index, year in enumerate(model_years):\n",
    "        # Calculate mass balance using the first element in MFA system (= base element)\n",
    "        # Negative value in process 0 means that process 0 has no inflows so this mass\n",
    "        # is coming from outside system boundaries\n",
    "        p0 = np.sum(mb[year_index][0][0])\n",
    "        rest = np.sum(mb[year_index][1:, 0])\n",
    "        abs_diff = abs(p0) - abs(rest)\n",
    "        df_mass_balance.loc[year_index] = np.array([year, p0, rest, abs_diff])\n",
    "    df_mass_balance = df_mass_balance.astype({\"Year\": \"int32\"})\n",
    "    df_mass_balance.set_index([\"Year\"], inplace=True)\n",
    "\n",
    "    # Show mass balance information for scenario\n",
    "    #print(df_mass_balance)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T09:39:36.783997600Z",
     "start_time": "2024-12-19T09:39:36.175176500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# *****************************************************\n",
    "# * Step 6: Plot the dynamic stocks results *\n",
    "# *****************************************************\n",
    "print(\"Show dynamic stock results...\")\n",
    "for scenario in scenarios:\n",
    "    # stock_id_to_dsm_swe and stock_id_to_dsm_carbon are dictionaries containing DynamicStockModel instances\n",
    "    flow_solver = scenario.flow_solver\n",
    "    stock_id_to_dsm_swe = flow_solver.get_dynamic_stocks_swe()\n",
    "    stock_id_to_dsm_carbon = flow_solver.get_dynamic_stocks_carbon()\n",
    "\n",
    "    if not len(stock_id_to_dsm_swe.keys()):\n",
    "        print(\"Scenario '{}': no dynamic stocks in the defined system\".format(scenario.name))\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(6, 1, sharex='all', sharey='none', figsize=(12, 20))\n",
    "\n",
    "    # Visualize stocks\n",
    "    for stock_id in stock_id_to_dsm_swe.keys():\n",
    "        # SWE Stock\n",
    "        dsm_swe = stock_id_to_dsm_swe[stock_id]\n",
    "        swe_stock_by_cohort = dsm_swe.compute_s_c_inflow_driven()\n",
    "        swe_oc = dsm_swe.compute_o_c_from_s_c()\n",
    "        swe_stock_total = dsm_swe.compute_stock_total()\n",
    "        swe_stock_change = dsm_swe.compute_stock_change()\n",
    "        swe_o = dsm_swe.compute_outflow_total()\n",
    "\n",
    "        # Carbon stock\n",
    "        dsm_carbon = stock_id_to_dsm_carbon[stock_id]\n",
    "        carbon_stock_by_cohort = dsm_carbon.compute_s_c_inflow_driven()\n",
    "        carbon_oc = dsm_carbon.compute_o_c_from_s_c()\n",
    "        carbon_stock_total = dsm_carbon.compute_stock_total()\n",
    "        carbon_stock_change = dsm_carbon.compute_stock_change()\n",
    "        carbon_o = dsm_carbon.compute_outflow_total()\n",
    "\n",
    "        # Plot SWE stock total (in-use stocks)\n",
    "        axes[0].plot(years, swe_stock_total, marker='o', label=f'SWE {stock_id}')\n",
    "        axes[0].set_ylabel(\"In-use stock (Mm3 SWE)\")\n",
    "        axes[0].set_title(\"In-use stock per year by product type\")\n",
    "\n",
    "        # Plot SWE stock change\n",
    "        axes[1].plot(years, swe_stock_change, marker='o', label=f'SWE {stock_id}')\n",
    "        axes[1].set_ylabel(\"Stock change (Mm3 SWE)\")\n",
    "        axes[1].set_title(\"Stock change per year by product type\")\n",
    "\n",
    "        # Plot SWE outflow by cohort\n",
    "        axes[2].plot(years, swe_o, marker='o', label=f'Mt carbon {stock_id}')\n",
    "        axes[2].set_ylabel(\"Stock outflow (Mm3 SWE)\")\n",
    "        axes[2].set_title(\"Stock outflow per year by product type\")\n",
    "\n",
    "        # Plot Carbon stock total (in-use stocks)\n",
    "        axes[3].plot(years, carbon_stock_total, marker='o', label=f'Mt carbon {stock_id}')\n",
    "        axes[3].set_ylabel(\"In-use stock (Mt C)\")\n",
    "        axes[3].set_title(\"Carbon stock in-use per year by product type\")\n",
    "\n",
    "        # Plot Carbon stock change\n",
    "        axes[4].plot(years, carbon_stock_change, marker='o', label=f'Mt carbon {stock_id}')\n",
    "        axes[4].set_ylabel(\"Stock change (Mt C)\")\n",
    "        axes[4].set_title(\"Carbon stock change per year\")\n",
    "\n",
    "        # Plot carbon outflow by cohort\n",
    "        axes[5].plot(years, carbon_o, marker='o', label=f'Mt carbon {stock_id}')\n",
    "        axes[5].set_ylabel(\"Carbon outflow (Mt C)\")\n",
    "        axes[5].set_title(\"Carbon outflow per year by product type\")\n",
    "\n",
    "    # Set common properties to axes\n",
    "    range_x_ticks = range(min(years), max(years) + 1)\n",
    "    for axis in axes:\n",
    "        axis.set_xlabel(\"Year\")\n",
    "        axis.title.set_size(12)\n",
    "        axis.legend()\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    tick_gap = 1 if len(years) < 15 else 10\n",
    "    plt.xticks(years[::tick_gap])\n",
    "\n",
    "    # Save the figure as an SVG file\n",
    "    svg_file_path = \"data/stock_plots_by_product.svg\"\n",
    "    plt.savefig(svg_file_path, format='svg')\n",
    "\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show CO2 removals graphs...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_dynamic_stocks_carbon'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m year_start \u001B[38;5;241m=\u001B[39m scenario_data\u001B[38;5;241m.\u001B[39mstart_year\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Calculate CO2 removals for each stock and plot the results\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m stock_id_to_dsm_carbon \u001B[38;5;241m=\u001B[39m \u001B[43mflow_solver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dynamic_stocks_carbon\u001B[49m()\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(stock_id_to_dsm_carbon\u001B[38;5;241m.\u001B[39mkeys()):\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mScenario \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: no dynamic stocks in the defined system\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(scenario\u001B[38;5;241m.\u001B[39mname))\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'get_dynamic_stocks_carbon'"
     ]
    }
   ],
   "source": [
    "# *****************************************************\n",
    "# * Step 7: Convert the carbon stocks to CO2 removals *\n",
    "# *****************************************************\n",
    "print(\"Show CO2 removals graphs...\")\n",
    "for scenario in scenarios:\n",
    "    scenario_data = scenario.scenario_data\n",
    "    flow_solver = scenario.flow_solver\n",
    "    years = scenario_data.years\n",
    "    year_start = scenario_data.start_year\n",
    "\n",
    "    # Calculate CO2 removals for each stock and plot the results\n",
    "    stock_id_to_dsm_carbon = flow_solver.get_dynamic_stocks_carbon()\n",
    "    if not len(stock_id_to_dsm_carbon.keys()):\n",
    "        print(\"Scenario '{}': no dynamic stocks in the defined system\".format(scenario.name))\n",
    "        continue\n",
    "\n",
    "    results_co2_removals = pd.DataFrame({'Year': years})\n",
    "    conversion_factor_c_to_co2 = model_params[ParameterName.ConversionFactorCToCO2]\n",
    "\n",
    "    # Define line styles, markers, and colors for differentiation\n",
    "    line_styles = ['-', '--', '-.', ':']\n",
    "    markers = ['o', 's', '^', 'D']\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for index, (stock_id, dsm_carbon) in enumerate(stock_id_to_dsm_carbon.items()):\n",
    "        total_inflows_carbon = dsm_carbon.i\n",
    "        total_outflows_carbon = dsm_carbon.o\n",
    "        annual_co2_removal = (total_inflows_carbon - total_outflows_carbon) * conversion_factor_c_to_co2\n",
    "        results_co2_removals[stock_id] = annual_co2_removal\n",
    "\n",
    "        line_style = line_styles[index % len(line_styles)]\n",
    "        marker = markers[index % len(markers)]\n",
    "        color = colors[index % len(colors)]\n",
    "        plt.plot(years, results_co2_removals[stock_id], marker=marker, linestyle=line_style, color=color, label=f'{stock_id} CO2 Removal')\n",
    "\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('CO2 Removal (Mt CO2)')\n",
    "    plt.title('Annual CO2 Removal by Product')\n",
    "    plt.grid(True)\n",
    "    tick_gap = 1 if len(years) < 15 else 10\n",
    "    plt.xticks(years[::tick_gap])\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure as an SVG file\n",
    "    svg_file_path = \"data/annual_co2_removal_by_product.svg\"\n",
    "    plt.savefig(svg_file_path, format='svg')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Annual CO2 removals (Mt) by stock:\\n\")\n",
    "    results_co2_removals.set_index(\"Year\", inplace=True)\n",
    "    print(results_co2_removals)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T09:39:37.493578100Z",
     "start_time": "2024-12-19T09:39:37.445381900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_processes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m flow_solver \u001B[38;5;241m=\u001B[39m scenario\u001B[38;5;241m.\u001B[39mflow_solver\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m process_id \u001B[38;5;129;01min\u001B[39;00m process_ids_to_visualize:\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m# Ignore visualizing inflows for processes that do not exist in this graph\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m process_id \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[43munique_processes\u001B[49m:\n\u001B[0;32m     11\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m     13\u001B[0m     flow_id_to_source_process_id \u001B[38;5;241m=\u001B[39m {}\n",
      "\u001B[1;31mNameError\u001B[0m: name 'unique_processes' is not defined"
     ]
    }
   ],
   "source": [
    "# ************************************************************\n",
    "# * Step 8: Visualize inflows per year to selected processes *\n",
    "# ************************************************************\n",
    "# Visualize inflows per year to processes\n",
    "process_ids_to_visualize = [\"Incineration:FI\", \"Sawmilling:FI\"]\n",
    "for scenario in scenarios:\n",
    "    flow_solver = scenario.flow_solver\n",
    "    for process_id in process_ids_to_visualize:\n",
    "        # Ignore visualizing inflows for processes that do not exist in this graph\n",
    "        if process_id not in unique_processes:\n",
    "            continue\n",
    "\n",
    "        flow_id_to_source_process_id = {}\n",
    "        source_process_names = []\n",
    "\n",
    "        # Find all source processes of all incoming flows to this process in all years\n",
    "        # This is needed to create stable set of process names so that the relative\n",
    "        # position of the processes stay the same in stacked chart between the years\n",
    "        source_process_ids = set()\n",
    "        for year in years:\n",
    "            inflows = flow_solver.get_process_flows(process_id, year)[\"Inflows\"]\n",
    "            unique_flow_ids = set()\n",
    "            for flow in inflows:\n",
    "                unique_flow_ids.add(flow.id)\n",
    "                flow_id_to_source_process_id[flow.id] = flow.source_process_id\n",
    "\n",
    "            # Find source process ID of each incoming flow and add\n",
    "            # to list of unique source process IDs if not already there\n",
    "            unique_flow_ids = list(unique_flow_ids)\n",
    "            for flow_id in unique_flow_ids:\n",
    "                source_process_ids.add(flow_id_to_source_process_id[flow_id])\n",
    "\n",
    "        # Now source_process_ids-list contains list of all the possible process IDs\n",
    "        # that have flows incoming to process_id. This list is needed to keep the\n",
    "        # incoming process IDs the same every year because aiphoria allows the connections\n",
    "        # between the flows to change between the years.\n",
    "        source_process_ids = list(source_process_ids)\n",
    "\n",
    "        # Create 2D array with shape of (number of source process IDs, number of years)\n",
    "        # and fill with the value of the inflow from source process for each year\n",
    "        source_process_by_flow_values = np.ndarray(shape=(len(source_process_ids), len(years)))\n",
    "        for year_index, year in enumerate(years):\n",
    "            inflows = flow_solver.get_process_flows(process_id, year)[\"Inflows\"]\n",
    "            for flow in inflows:\n",
    "                source_process_id_index = source_process_ids.index(flow.source_process_id)\n",
    "                source_process_by_flow_values[source_process_id_index, year_index] = flow.evaluated_value\n",
    "                print(f\"Source process ID: {flow.source_process_id}, Input value: {flow.evaluated_value}, Year: {year}\")\n",
    "\n",
    "        # Get the process name for the currently visualized process\n",
    "        process = flow_solver.get_process(process_id, min(years))\n",
    "\n",
    "        # Initialize the figure and axes for the stacked area chart\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        ax.stackplot(years, source_process_by_flow_values, labels=list(source_process_ids))\n",
    "        ax.set_ylabel(\"Mm3 SWE\")\n",
    "        ax.set_title(\"Inputs to {}\".format(process.name))\n",
    "        ax.legend(loc='upper left')\n",
    "        tick_gap = 1 if len(years) < 15 else 10\n",
    "        plt.xticks(years[::tick_gap])\n",
    "\n",
    "        # Save the figure as an SVG file\n",
    "        svg_file_path = \"data/inflows_to_processes_{}.svg\".format(process.name)\n",
    "        plt.savefig(svg_file_path, format='svg')\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T09:39:38.148353Z",
     "start_time": "2024-12-19T09:39:38.100354300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_year_to_process_to_flows'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 54\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_params[ParameterName\u001B[38;5;241m.\u001B[39mCreateSankeyCharts]:\n\u001B[0;32m     53\u001B[0m     visualizer \u001B[38;5;241m=\u001B[39m DataVisualizer()\n\u001B[1;32m---> 54\u001B[0m     \u001B[43mvisualizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_and_show\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscenarios\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualizer_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\dev\\PythonProjects\\aiphoria\\core\\datavisualizer.py:322\u001B[0m, in \u001B[0;36mDataVisualizer.build_and_show\u001B[1;34m(self, scenarios, params, combine_to_one_file)\u001B[0m\n\u001B[0;32m    320\u001B[0m scenario_name_to_data \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    321\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m scenario \u001B[38;5;129;01min\u001B[39;00m scenarios:\n\u001B[1;32m--> 322\u001B[0m     scenario_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_build_scenario_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscenario\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    323\u001B[0m     scenario_name_to_data[scenario\u001B[38;5;241m.\u001B[39mname] \u001B[38;5;241m=\u001B[39m scenario_data\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m combine_to_one_file:\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;66;03m# Build combined file with all the scenarios included\u001B[39;00m\n",
      "File \u001B[1;32mC:\\dev\\PythonProjects\\aiphoria\\core\\datavisualizer.py:585\u001B[0m, in \u001B[0;36mDataVisualizer._build_scenario_data\u001B[1;34m(self, scenario, params)\u001B[0m\n\u001B[0;32m    582\u001B[0m virtual_flow_color \u001B[38;5;241m=\u001B[39m params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvirtual_flow_color\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    584\u001B[0m year_to_data \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m--> 585\u001B[0m year_to_process_to_flows \u001B[38;5;241m=\u001B[39m \u001B[43mflow_solver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_year_to_process_to_flows\u001B[49m()\n\u001B[0;32m    586\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m year, process_to_flows \u001B[38;5;129;01min\u001B[39;00m year_to_process_to_flows\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    587\u001B[0m     year_to_data[year] \u001B[38;5;241m=\u001B[39m {}\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'get_year_to_process_to_flows'"
     ]
    }
   ],
   "source": [
    "# *****************************************************\n",
    "# * Step 9: Visualize the flow graph as Sankey graph *\n",
    "# *****************************************************\n",
    "\n",
    "# Color mappings\n",
    "process_transformation_stage_colors = dict()\n",
    "process_transformation_stage_colors[\"Source\"] = \"#7DDA60\"\n",
    "process_transformation_stage_colors[\"First\"] = \"#eb5e34\"\n",
    "process_transformation_stage_colors[\"Second\"] = \"#8c76cf\"\n",
    "process_transformation_stage_colors[\"Third\"] = \"#5BAA11\"\n",
    "process_transformation_stage_colors[\"VAM\"] = \"#3281db\"\n",
    "process_transformation_stage_colors[\"RoW\"] = \"#61b053\"  # Rest of the world\n",
    "process_transformation_stage_colors[\"EoL\"] = \"#EFC3CA\"  # Brown\n",
    "process_transformation_stage_colors[\"by_prod\"] = \"#DFC57B\"  # gold\n",
    "process_transformation_stage_colors[\"Virtual\"] = \"#707070\"\n",
    "process_transformation_stage_colors[\"Other\"] = \"#707070\"\n",
    "\n",
    "# Label overrides\n",
    "virtual_process_graph_labels = dict()\n",
    "virtual_process_graph_labels[\"VP_P2:EU\"] = \"Unreported flow from P2\"\n",
    "virtual_process_graph_labels[\"VP_P3:EU\"] = \"Unreported flow from P3\"\n",
    "\n",
    "# Virtual Process and virtual Flow colors\n",
    "visualizer_params = {\n",
    "    # User can hide processes in Sankey graph that have total inflows less than this value\n",
    "    # This value cannot be changed now in the Sankey graph\n",
    "    \"small_node_threshold\": 5,\n",
    "\n",
    "    # Dictionary to define labels for virtual flows\n",
    "    # If dictionary contains label for the virtual process then that is used,\n",
    "    # otherwise the virtual process ID is used\n",
    "    \"virtual_process_graph_labels\": virtual_process_graph_labels,\n",
    "\n",
    "    # Dictionary to define color of process by the process transformation stage name\n",
    "    # All must be provided as a RGB hex string, prefixed by character '#'\n",
    "    # Usage example: { \"Source\": \"#707070\" }\n",
    "    \"process_transformation_stage_colors\": process_transformation_stage_colors,\n",
    "\n",
    "    # How transparent flows are (0.0 = invisible, 1.0 = fully opaque)\n",
    "    \"flow_alpha\": 0.75,\n",
    "\n",
    "    # Color for virtual process\n",
    "    \"virtual_process_color\": \"rgba(0.3, 0.3, 0.3, 0.6)\",\n",
    "    #\"virtual_process_color\": \"#707070\",\n",
    "\n",
    "    # Color for virtual flows\n",
    "    #\"virtual_flow_color\": \"rgba(0.5, 0.5, 0.5, 0.5)\",\n",
    "    \"virtual_flow_color\": \"#808080\",\n",
    "}\n",
    "\n",
    "# # NOTE: Now each scenario is in it's own graph\n",
    "if model_params[ParameterName.CreateSankeyCharts]:\n",
    "    visualizer = DataVisualizer()\n",
    "    visualizer.build_and_show(scenarios, visualizer_params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T09:39:38.839831800Z",
     "start_time": "2024-12-19T09:39:38.759410800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-19T09:38:44.826816500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
