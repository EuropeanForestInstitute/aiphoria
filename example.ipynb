{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# %%\n",
    "# *****************************************************\n",
    "# * Import required packages and set up path for ODYM *\n",
    "# *****************************************************\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from core.dataprovider import DataProvider\n",
    "from core.datachecker import DataChecker\n",
    "from core.datastructures import Scenario\n",
    "from core.datavisualizer import DataVisualizer\n",
    "from core.network_graph import NetworkGraph\n",
    "from core.flowsolver import FlowSolver\n",
    "from core.parameters import ParameterName\n",
    "\n",
    "# Path configuration\n",
    "sys.path.insert(0, os.path.join('.'))\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '.', 'lib', 'odym', 'modules'))\n",
    "\n",
    "# ODYM classes\n",
    "import ODYM_Classes as msc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-17T11:50:36.431565400Z",
     "start_time": "2024-12-17T11:50:36.373653100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file data/example_data_scenarios.xlsx...\n",
      "FunctionType.Constant\n",
      "FunctionType.Linear\n",
      "FunctionType.Exponential\n",
      "FunctionType.Sigmoid\n",
      "FunctionType.Constant\n",
      "FunctionType.Linear\n",
      "FunctionType.Exponential\n",
      "FunctionType.Sigmoid\n",
      "Using following parameters for running the model:\n",
      "\tsheet_name_processes            = Processes\n",
      "\tcolumn_range_processes          = B:R\n",
      "\tskip_num_rows_processes         = 2\n",
      "\tsheet_name_flows                = Flows\n",
      "\tcolumn_range_flows              = B:U\n",
      "\tskip_num_rows_flows             = 2\n",
      "\tstart_year                      = 2000\n",
      "\tend_year                        = 2010\n",
      "\tdetect_year_range               = False\n",
      "\tuse_virtual_flows               = True\n",
      "\tvirtual_flows_epsilon           = 0.1\n",
      "\tconversion_factor_c_to_co2      = 3.67\n",
      "\tfill_missing_absolute_flows     = True\n",
      "\tfill_missing_relative_flows     = True\n",
      "\tfill_method                     = Previous\n",
      "\tsheet_name_scenarios            = Scenarios\n"
     ]
    }
   ],
   "source": [
    "# ******************************************************\n",
    "# * Step 1: Load the data from Excel file DataProvider *\n",
    "# ******************************************************\n",
    "\n",
    "# Load data from file using the DataProvider\n",
    "# and get model parameters from the file\n",
    "filename = \"data/example_data_scenarios.xlsx\"\n",
    "#filename = \"data/20240917_JWEE_use.xlsx\"\n",
    "\n",
    "print(\"Loading data from file {}...\".format(filename))\n",
    "dataprovider = DataProvider(filename)\n",
    "\n",
    "# Model parameters is a Dictionary that contains loaded data from Excel sheet named \"Settings\"\n",
    "# and are used for running the FlowSolver and setting up ODYM\n",
    "model_params = dataprovider.get_model_params()\n",
    "print(\"Using following parameters for running the model:\")\n",
    "for param_name, param_value in model_params.items():\n",
    "    print(\"\\t{:32}= {}\".format(param_name, param_value))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-17T11:50:36.747724400Z",
     "start_time": "2024-12-17T11:50:36.431565400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking errors in data...\n",
      "Checking stock distribution types...\n",
      "Checking stock distribution parameters...\n",
      "Checking process total inflows and total outflows mismatches...\n",
      "Checking flow type changes...\n",
      "Checking for processes that have no inflows and only relative outflows...\n",
      "Checking scenario definitions...\n",
      "FunctionType.Linear\n",
      "FunctionType.Linear\n",
      "Building baseline scenario...\n",
      "Building 2 alternative scenarios...\n"
     ]
    }
   ],
   "source": [
    "# ***********************************************************************\n",
    "# * Step 2: Check for errors in data and build scenarios for FlowSolver *\n",
    "# ***********************************************************************\n",
    "\n",
    "print(\"Checking errors in data...\")\n",
    "data_checker = DataChecker(dataprovider)\n",
    "scenarios = data_checker.build_scenarios()\n",
    "is_checker_ok, checker_messages = data_checker.check_for_errors()\n",
    "if not is_checker_ok:\n",
    "    for msg in checker_messages:\n",
    "        print(msg)\n",
    "    SystemExit(-1)\n",
    "\n",
    "# Create network graph for data\n",
    "show_network_graph = False\n",
    "if show_network_graph:\n",
    "    # scenarios[0] is always the baseline scenario and is guaranteed to exist\n",
    "    network_visualizer = NetworkGraph()\n",
    "    network_visualizer.build(scenarios[0].scenario_data)\n",
    "    network_visualizer.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-17T11:50:36.848040700Z",
     "start_time": "2024-12-17T11:50:36.747724400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving flows for year 2010/2010: : 11it [00:00, 704.05it/s]\n",
      "Solving flows for year 2010/2010: : 11it [00:00, 704.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Applying flow modifiers for scenario 'Virtual inflow' ***\n",
      "2006: Create virtual inflow\n",
      "2007: Create virtual inflow\n",
      "2008: Create virtual inflow\n",
      "2009: Create virtual inflow\n",
      "2010: Create virtual inflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving flows for year 2007/2010: : 7it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Applying flow modifiers for scenario 'Virtual outflow' ***\n",
      "Created 1 virtual processes and 1 virtual flows for year 2001\n",
      "\t- Virtual process ID 'VP_P1:FI'\n",
      "\t- Virtual flow ID 'P1:FI VP_P1:FI'\n",
      "\n",
      "Created 1 virtual processes and 1 virtual flows for year 2002\n",
      "\t- Virtual process ID 'VP_P1:FI'\n",
      "\t- Virtual flow ID 'P1:FI VP_P1:FI'\n",
      "\n",
      "Created 1 virtual processes and 1 virtual flows for year 2003\n",
      "\t- Virtual process ID 'VP_P1:FI'\n",
      "\t- Virtual flow ID 'P1:FI VP_P1:FI'\n",
      "\n",
      "Created 1 virtual processes and 1 virtual flows for year 2004\n",
      "\t- Virtual process ID 'VP_P1:FI'\n",
      "\t- Virtual flow ID 'P1:FI VP_P1:FI'\n",
      "\n",
      "Created 1 virtual processes and 1 virtual flows for year 2005\n",
      "\t- Virtual process ID 'VP_P1:FI'\n",
      "\t- Virtual flow ID 'P1:FI VP_P1:FI'\n",
      "\n",
      "Created 1 virtual processes and 1 virtual flows for year 2006\n",
      "\t- Virtual process ID 'VP_P1:FI'\n",
      "\t- Virtual flow ID 'P1:FI VP_P1:FI'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving flows for year 2010/2010: : 11it [00:00, 703.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1 virtual processes and 1 virtual flows for year 2007\n",
      "\t- Virtual process ID 'VP_P1:FI'\n",
      "\t- Virtual flow ID 'P1:FI VP_P1:FI'\n",
      "\n",
      "Created 1 virtual processes and 1 virtual flows for year 2008\n",
      "\t- Virtual process ID 'VP_P1:FI'\n",
      "\t- Virtual flow ID 'P1:FI VP_P1:FI'\n",
      "\n",
      "Created 1 virtual processes and 1 virtual flows for year 2009\n",
      "\t- Virtual process ID 'VP_P1:FI'\n",
      "\t- Virtual flow ID 'P1:FI VP_P1:FI'\n",
      "\n",
      "Created 1 virtual processes and 1 virtual flows for year 2010\n",
      "\t- Virtual process ID 'VP_P1:FI'\n",
      "\t- Virtual flow ID 'P1:FI VP_P1:FI'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ******************************************************************\n",
    "# * Step 3: Solve flows for baseline scenario using the FlowSolver *\n",
    "# ******************************************************************\n",
    "for scenario_index, scenario in enumerate(scenarios):\n",
    "    # NOTE: Baseline scenario is always the first element in the list\n",
    "    # and all the alternative scenarios (if any) are after that\n",
    "    if scenario_index == 0:\n",
    "        # Process baseline scenario\n",
    "        baseline_flow_solver = FlowSolver(scenario=scenario)\n",
    "        baseline_flow_solver.solve_timesteps()\n",
    "        scenario.flow_solver = baseline_flow_solver\n",
    "    else:\n",
    "        # Get and copy solved scenario data from baseline scenario flow solver\n",
    "        baseline_scenario_data = scenarios[0].flow_solver.get_solved_scenario_data()\n",
    "        scenario.copy_from_baseline_scenario_data(baseline_scenario_data)\n",
    "\n",
    "        # Solve this alternative scenario time steps\n",
    "        scenario_flow_solver = FlowSolver(scenario=scenario)\n",
    "        scenario_flow_solver.solve_timesteps()\n",
    "        scenario.flow_solver = scenario_flow_solver"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-17T11:50:36.995203300Z",
     "start_time": "2024-12-17T11:50:36.848040700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ODYM MFA for scenario 'Baseline' (1/3)...\n",
      "Creating ODYM objects...\n",
      "Building ODYM processes...\n",
      "Building ODYM flows...\n",
      "Building ODYM stocks...\n",
      "Mass balance difference per year\n",
      "Mass balance result shape: (11, 5, 2)\n",
      "Building ODYM MFA for scenario 'Virtual inflow' (2/3)...\n",
      "Creating ODYM objects...\n",
      "Building ODYM processes...\n",
      "Building ODYM flows...\n",
      "Building ODYM stocks...\n",
      "Mass balance difference per year\n",
      "Mass balance result shape: (11, 6, 2)\n",
      "Building ODYM MFA for scenario 'Virtual outflow' (3/3)...\n",
      "Creating ODYM objects...\n",
      "Building ODYM processes...\n",
      "Building ODYM flows...\n",
      "Building ODYM stocks...\n",
      "Mass balance difference per year\n",
      "Mass balance result shape: (11, 6, 2)\n"
     ]
    }
   ],
   "source": [
    "# ************************************************************************\n",
    "# * Step 4: Setup ODYM classifications and index table for each Scenario *\n",
    "# ************************************************************************\n",
    "for scenario_index, scenario in enumerate(scenarios):\n",
    "    print(\"Building ODYM MFA for scenario '{}' ({}/{})...\".format(scenario.name, scenario_index + 1, len(scenarios)))\n",
    "    # NOTE: ODYM MFA is only used for mass balance check?\n",
    "\n",
    "    # Track solid wood equivalent and carbon.\n",
    "    # Dictionary of classifications enters the index table defined for the system.\n",
    "    # The index table lists all aspects needed and assigns a classification and index letter to each aspect.\n",
    "    scenario_data = scenario.scenario_data\n",
    "    years = scenario_data.years\n",
    "\n",
    "    model_time_start = scenario_data.start_year\n",
    "    model_time_end = scenario_data.end_year\n",
    "    model_elements = ['Solid wood equivalent', 'Carbon']\n",
    "    model_years = years\n",
    "\n",
    "    model_classifications = {\n",
    "        'Time': msc.Classification(Name='Time', Dimension='Time', ID=1, Items=model_years),\n",
    "        'Cohort': msc.Classification(Name='Age-cohort', Dimension='Time', ID=2, Items=model_years),\n",
    "        'Element': msc.Classification(Name='Elements', Dimension='Element', ID=3, Items=model_elements),\n",
    "    }\n",
    "\n",
    "    index_table = pd.DataFrame({'Aspect': ['Time', 'Age-cohort', 'Element'],  # 'Time' and 'Element' must be present!\n",
    "                                'Description': ['Model aspect \"time\"', 'Model aspect \"age-cohort\"', 'Model aspect \"Element\"'],\n",
    "                                'Dimension': ['Time', 'Time', 'Element'],  # 'Time' and 'Element' are also dimensions\n",
    "                                'Classification': [model_classifications[Aspect] for Aspect in ['Time', 'Cohort', 'Element']],\n",
    "                                'IndexLetter': ['t', 'c', 'e' ]})  # Unique one letter (upper or lower case) indices to be used later for calculations.\n",
    "\n",
    "    index_table.set_index('Aspect', inplace=True)  # Default indexing of IndexTable, other indices are produced on the fly\n",
    "    index_table\n",
    "\n",
    "    # **************************************\n",
    "    # * Step 5: Initialize ODYM MFA system *\n",
    "    # **************************************\n",
    "    flow_solver = scenario.flow_solver\n",
    "\n",
    "    mfa_system = msc.MFAsystem(Name='Wood product system', Geogr_Scope='Europe', Unit='Mm3',\n",
    "                               ProcessList=[], FlowDict={}, StockDict={}, ParameterDict={},\n",
    "                               Time_Start=model_time_start, Time_End=model_time_end, IndexTable=index_table,\n",
    "                               Elements=index_table.loc['Element'].Classification.Items)\n",
    "\n",
    "    # Get inflow values to stock\n",
    "    year_index_to_year = dict(enumerate(model_years))\n",
    "    unique_processes = flow_solver.get_unique_processes()\n",
    "    unique_flows = flow_solver.get_unique_flows()\n",
    "\n",
    "    # NOTE: These are not used anywhere\n",
    "    # # DataFrames for Processes, Flows and Flow values\n",
    "    # print(\"Collecting processes to DataFrame...\")\n",
    "    # df_processes = flow_solver.get_processes_as_dataframe()\n",
    "    # print(\"Collecting flows to DataFrame...\")\n",
    "    # df_flows = flow_solver.get_flows_as_dataframe()\n",
    "    # print(\"Collecting evaluated flow values to DataFrame...\")\n",
    "    # df_flow_values = flow_solver.get_evaluated_flow_values_as_dataframe()\n",
    "\n",
    "    print(\"Creating ODYM objects...\")\n",
    "    # Create ODYM objects\n",
    "\n",
    "    print(\"Building ODYM processes...\")\n",
    "    odym_processes = []\n",
    "    process_id_to_index = {}\n",
    "    for process_id, process in unique_processes.items():\n",
    "        process_index = len(odym_processes)\n",
    "        process_id_to_index[process_id] = process_index\n",
    "        new_process = msc.Process(ID=process_index, Name=process.name)\n",
    "        odym_processes.append(new_process)\n",
    "\n",
    "    print(\"Building ODYM flows...\")\n",
    "    odym_flows = {}\n",
    "    for flow_id, flow in unique_flows.items():\n",
    "        source_process_index = process_id_to_index[flow.source_process_id]\n",
    "        target_process_index = process_id_to_index[flow.target_process_id]\n",
    "        new_flow = msc.Flow(ID=flow.id, P_Start=source_process_index, P_End=target_process_index, Indices='t,e', Values=None)\n",
    "        odym_flows[flow.id] = new_flow\n",
    "\n",
    "    print(\"Building ODYM stocks...\")\n",
    "    odym_stocks = {}\n",
    "    for stock in flow_solver.get_all_stocks():\n",
    "        process_index = process_id_to_index[stock.id]\n",
    "        new_stock = msc.Stock(ID=stock.id, Name=stock.name, P_Res=process_index, Indices='t,e', Type=0, Values=None)\n",
    "        odym_stocks[stock.id] = new_stock\n",
    "\n",
    "    mfa_system.ProcessList = odym_processes\n",
    "    mfa_system.FlowDict = odym_flows\n",
    "    mfa_system.StockDict = odym_stocks\n",
    "    mfa_system.Initialize_FlowValues()\n",
    "    mfa_system.Initialize_StockValues()\n",
    "    mfa_system.Consistency_Check()\n",
    "\n",
    "    # Update ODYM flow values from flow values DataFrame\n",
    "    for flow_id, flow in mfa_system.FlowDict.items():\n",
    "        for year_index, value in enumerate(flow.Values):\n",
    "            # Skip to next year if FlowSolver does not have data for this year\n",
    "            # This is possible because ODYM flow and stock values are already initialized to 0.0\n",
    "            flow_has_data_for_year = flow_solver.has_flow(year=year_index_to_year[year_index], flow_id=flow_id)\n",
    "            if not flow_has_data_for_year:\n",
    "                continue\n",
    "\n",
    "            # NOTE: Virtual flows use default value defined in Flow for carbon content (now 1.0).\n",
    "            solved_flow = flow_solver.get_flow(year=year_index_to_year[year_index], flow_id=flow_id)\n",
    "            flow.Values[year_index, 0] = solved_flow.evaluated_value\n",
    "            flow.Values[year_index, 1] = solved_flow.evaluated_value_carbon\n",
    "\n",
    "    # Process stocks (fill with data)\n",
    "    for stock_id, stock in odym_stocks.items():\n",
    "        # Calculate cohorts for \"Solid wood equivalent\"\n",
    "        dsm_swe = flow_solver.get_dynamic_stocks_swe()[stock_id]\n",
    "        swe_stock_by_cohort = dsm_swe.compute_s_c_inflow_driven()\n",
    "        swe_outflow_by_cohort = dsm_swe.compute_o_c_from_s_c()\n",
    "        swe_stock_total = dsm_swe.compute_stock_total()\n",
    "        swe_stock_change = dsm_swe.compute_stock_change()\n",
    "        stock.Values[:, 0] = swe_stock_change\n",
    "\n",
    "        # Calculate cohorts for \"Carbon\"\n",
    "        dsm_carbon = flow_solver.get_dynamic_stocks_carbon()[stock_id]\n",
    "        carbon_stock_by_cohort = dsm_carbon.compute_s_c_inflow_driven()\n",
    "        carbon_outflow_by_cohort = dsm_carbon.compute_o_c_from_s_c()\n",
    "        carbon_stock_total = dsm_carbon.compute_stock_total()\n",
    "        carbon_stock_change = dsm_carbon.compute_stock_change()\n",
    "        stock.Values[:, 1] = carbon_stock_change\n",
    "\n",
    "    print(\"Mass balance difference per year\")\n",
    "    mb = mfa_system.MassBalance()\n",
    "    print(\"Mass balance result shape: {}\".format(mb.shape))\n",
    "    df_mass_balance = pd.DataFrame(columns=[\"Year\", \"Process 0\", \"Rest\", \"Abs difference\"])\n",
    "    for year_index, year in enumerate(model_years):\n",
    "        # Calculate mass balance using the first element in MFA system (= base element)\n",
    "        # Negative value in process 0 means that process 0 has no inflows so this mass\n",
    "        # is coming from outside system boundaries\n",
    "        p0 = np.sum(mb[year_index][0][0])\n",
    "        rest = np.sum(mb[year_index][1:, 0])\n",
    "        abs_diff = abs(p0) - abs(rest)\n",
    "        df_mass_balance.loc[year_index] = np.array([year, p0, rest, abs_diff])\n",
    "    df_mass_balance = df_mass_balance.astype({\"Year\": \"int32\"})\n",
    "    df_mass_balance.set_index([\"Year\"], inplace=True)\n",
    "\n",
    "    # Show mass balance information for scenario\n",
    "    #print(df_mass_balance)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-17T11:50:37.064230800Z",
     "start_time": "2024-12-17T11:50:36.948328300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show dynamic stock results...\n",
      "Scenario 'Baseline': no dynamic stocks in the defined system\n",
      "Scenario 'Virtual inflow': no dynamic stocks in the defined system\n",
      "Scenario 'Virtual outflow': no dynamic stocks in the defined system\n"
     ]
    }
   ],
   "source": [
    "# *****************************************************\n",
    "# * Step 6: Plot the dynamic stocks results *\n",
    "# *****************************************************\n",
    "print(\"Show dynamic stock results...\")\n",
    "for scenario in scenarios:\n",
    "    # stock_id_to_dsm_swe and stock_id_to_dsm_carbon are dictionaries containing DynamicStockModel instances\n",
    "    flow_solver = scenario.flow_solver\n",
    "    stock_id_to_dsm_swe = flow_solver.get_dynamic_stocks_swe()\n",
    "    stock_id_to_dsm_carbon = flow_solver.get_dynamic_stocks_carbon()\n",
    "\n",
    "    if not len(stock_id_to_dsm_swe.keys()):\n",
    "        print(\"Scenario '{}': no dynamic stocks in the defined system\".format(scenario.name))\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(6, 1, sharex='all', sharey='none', figsize=(12, 20))\n",
    "\n",
    "    # Visualize stocks\n",
    "    for stock_id in stock_id_to_dsm_swe.keys():\n",
    "        # SWE Stock\n",
    "        dsm_swe = stock_id_to_dsm_swe[stock_id]\n",
    "        swe_stock_by_cohort = dsm_swe.compute_s_c_inflow_driven()\n",
    "        swe_oc = dsm_swe.compute_o_c_from_s_c()\n",
    "        swe_stock_total = dsm_swe.compute_stock_total()\n",
    "        swe_stock_change = dsm_swe.compute_stock_change()\n",
    "        swe_o = dsm_swe.compute_outflow_total()\n",
    "\n",
    "        # Carbon stock\n",
    "        dsm_carbon = stock_id_to_dsm_carbon[stock_id]\n",
    "        carbon_stock_by_cohort = dsm_carbon.compute_s_c_inflow_driven()\n",
    "        carbon_oc = dsm_carbon.compute_o_c_from_s_c()\n",
    "        carbon_stock_total = dsm_carbon.compute_stock_total()\n",
    "        carbon_stock_change = dsm_carbon.compute_stock_change()\n",
    "        carbon_o = dsm_carbon.compute_outflow_total()\n",
    "\n",
    "        # Plot SWE stock total (in-use stocks)\n",
    "        axes[0].plot(years, swe_stock_total, marker='o', label=f'SWE {stock_id}')\n",
    "        axes[0].set_ylabel(\"In-use stock (Mm3 SWE)\")\n",
    "        axes[0].set_title(\"In-use stock per year by product type\")\n",
    "\n",
    "        # Plot SWE stock change\n",
    "        axes[1].plot(years, swe_stock_change, marker='o', label=f'SWE {stock_id}')\n",
    "        axes[1].set_ylabel(\"Stock change (Mm3 SWE)\")\n",
    "        axes[1].set_title(\"Stock change per year by product type\")\n",
    "\n",
    "        # Plot SWE outflow by cohort\n",
    "        axes[2].plot(years, swe_o, marker='o', label=f'Mt carbon {stock_id}')\n",
    "        axes[2].set_ylabel(\"Stock outflow (Mm3 SWE)\")\n",
    "        axes[2].set_title(\"Stock outflow per year by product type\")\n",
    "\n",
    "        # Plot Carbon stock total (in-use stocks)\n",
    "        axes[3].plot(years, carbon_stock_total, marker='o', label=f'Mt carbon {stock_id}')\n",
    "        axes[3].set_ylabel(\"In-use stock (Mt C)\")\n",
    "        axes[3].set_title(\"Carbon stock in-use per year by product type\")\n",
    "\n",
    "        # Plot Carbon stock change\n",
    "        axes[4].plot(years, carbon_stock_change, marker='o', label=f'Mt carbon {stock_id}')\n",
    "        axes[4].set_ylabel(\"Stock change (Mt C)\")\n",
    "        axes[4].set_title(\"Carbon stock change per year\")\n",
    "\n",
    "        # Plot carbon outflow by cohort\n",
    "        axes[5].plot(years, carbon_o, marker='o', label=f'Mt carbon {stock_id}')\n",
    "        axes[5].set_ylabel(\"Carbon outflow (Mt C)\")\n",
    "        axes[5].set_title(\"Carbon outflow per year by product type\")\n",
    "\n",
    "    # Set common properties to axes\n",
    "    range_x_ticks = range(min(years), max(years) + 1)\n",
    "    for axis in axes:\n",
    "        axis.set_xlabel(\"Year\")\n",
    "        axis.title.set_size(12)\n",
    "        axis.legend()\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    tick_gap = 1 if len(years) < 15 else 10\n",
    "    plt.xticks(years[::tick_gap])\n",
    "\n",
    "    # Save the figure as an SVG file\n",
    "    svg_file_path = \"data/stock_plots_by_product.svg\"\n",
    "    plt.savefig(svg_file_path, format='svg')\n",
    "\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-17T11:50:37.064230800Z",
     "start_time": "2024-12-17T11:50:37.064230800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show CO2 removals graphs...\n",
      "Scenario 'Baseline': no dynamic stocks in the defined system\n",
      "Scenario 'Virtual inflow': no dynamic stocks in the defined system\n",
      "Scenario 'Virtual outflow': no dynamic stocks in the defined system\n"
     ]
    }
   ],
   "source": [
    "# *****************************************************\n",
    "# * Step 7: Convert the carbon stocks to CO2 removals *\n",
    "# *****************************************************\n",
    "print(\"Show CO2 removals graphs...\")\n",
    "for scenario in scenarios:\n",
    "    scenario_data = scenario.scenario_data\n",
    "    flow_solver = scenario.flow_solver\n",
    "    years = scenario_data.years\n",
    "    year_start = scenario_data.start_year\n",
    "\n",
    "    # Calculate CO2 removals for each stock and plot the results\n",
    "    stock_id_to_dsm_carbon = flow_solver.get_dynamic_stocks_carbon()\n",
    "    if not len(stock_id_to_dsm_carbon.keys()):\n",
    "        print(\"Scenario '{}': no dynamic stocks in the defined system\".format(scenario.name))\n",
    "        continue\n",
    "\n",
    "    results_co2_removals = pd.DataFrame({'Year': years})\n",
    "    conversion_factor_c_to_co2 = model_params[ParameterName.ConversionFactorCToCO2]\n",
    "\n",
    "    # Define line styles, markers, and colors for differentiation\n",
    "    line_styles = ['-', '--', '-.', ':']\n",
    "    markers = ['o', 's', '^', 'D']\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for index, (stock_id, dsm_carbon) in enumerate(stock_id_to_dsm_carbon.items()):\n",
    "        total_inflows_carbon = dsm_carbon.i\n",
    "        total_outflows_carbon = dsm_carbon.o\n",
    "        annual_co2_removal = (total_inflows_carbon - total_outflows_carbon) * conversion_factor_c_to_co2\n",
    "        results_co2_removals[stock_id] = annual_co2_removal\n",
    "\n",
    "        line_style = line_styles[index % len(line_styles)]\n",
    "        marker = markers[index % len(markers)]\n",
    "        color = colors[index % len(colors)]\n",
    "        plt.plot(years, results_co2_removals[stock_id], marker=marker, linestyle=line_style, color=color, label=f'{stock_id} CO2 Removal')\n",
    "\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('CO2 Removal (Mt CO2)')\n",
    "    plt.title('Annual CO2 Removal by Product')\n",
    "    plt.grid(True)\n",
    "    tick_gap = 1 if len(years) < 15 else 10\n",
    "    plt.xticks(years[::tick_gap])\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure as an SVG file\n",
    "    svg_file_path = \"data/annual_co2_removal_by_product.svg\"\n",
    "    plt.savefig(svg_file_path, format='svg')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Annual CO2 removals (Mt) by stock:\\n\")\n",
    "    results_co2_removals.set_index(\"Year\", inplace=True)\n",
    "    print(results_co2_removals)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-17T11:50:37.064230800Z",
     "start_time": "2024-12-17T11:50:37.064230800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# ************************************************************\n",
    "# * Step 8: Visualize inflows per year to selected processes *\n",
    "# ************************************************************\n",
    "# Visualize inflows per year to processes\n",
    "process_ids_to_visualize = [\"Incineration:FI\", \"Sawmilling:FI\"]\n",
    "for scenario in scenarios:\n",
    "    flow_solver = scenario.flow_solver\n",
    "    for process_id in process_ids_to_visualize:\n",
    "        # Ignore visualizing inflows for processes that do not exist in this graph\n",
    "        if process_id not in unique_processes:\n",
    "            continue\n",
    "\n",
    "        flow_id_to_source_process_id = {}\n",
    "        source_process_names = []\n",
    "\n",
    "        # Find all source processes of all incoming flows to this process in all years\n",
    "        # This is needed to create stable set of process names so that the relative\n",
    "        # position of the processes stay the same in stacked chart between the years\n",
    "        source_process_ids = set()\n",
    "        for year in years:\n",
    "            inflows = flow_solver.get_process_flows(process_id, year)[\"Inflows\"]\n",
    "            unique_flow_ids = set()\n",
    "            for flow in inflows:\n",
    "                unique_flow_ids.add(flow.id)\n",
    "                flow_id_to_source_process_id[flow.id] = flow.source_process_id\n",
    "\n",
    "            # Find source process ID of each incoming flow and add\n",
    "            # to list of unique source process IDs if not already there\n",
    "            unique_flow_ids = list(unique_flow_ids)\n",
    "            for flow_id in unique_flow_ids:\n",
    "                source_process_ids.add(flow_id_to_source_process_id[flow_id])\n",
    "\n",
    "        # Now source_process_ids-list contains list of all the possible process IDs\n",
    "        # that have flows incoming to process_id. This list is needed to keep the\n",
    "        # incoming process IDs the same every year because aiphoria allows the connections\n",
    "        # between the flows to change between the years.\n",
    "        source_process_ids = list(source_process_ids)\n",
    "\n",
    "        # Create 2D array with shape of (number of source process IDs, number of years)\n",
    "        # and fill with the value of the inflow from source process for each year\n",
    "        source_process_by_flow_values = np.ndarray(shape=(len(source_process_ids), len(years)))\n",
    "        for year_index, year in enumerate(years):\n",
    "            inflows = flow_solver.get_process_flows(process_id, year)[\"Inflows\"]\n",
    "            for flow in inflows:\n",
    "                source_process_id_index = source_process_ids.index(flow.source_process_id)\n",
    "                source_process_by_flow_values[source_process_id_index, year_index] = flow.evaluated_value\n",
    "                print(f\"Source process ID: {flow.source_process_id}, Input value: {flow.evaluated_value}, Year: {year}\")\n",
    "\n",
    "        # Get the process name for the currently visualized process\n",
    "        process = flow_solver.get_process(process_id, min(years))\n",
    "\n",
    "        # Initialize the figure and axes for the stacked area chart\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        ax.stackplot(years, source_process_by_flow_values, labels=list(source_process_ids))\n",
    "        ax.set_ylabel(\"Mm3 SWE\")\n",
    "        ax.set_title(\"Inputs to {}\".format(process.name))\n",
    "        ax.legend(loc='upper left')\n",
    "        tick_gap = 1 if len(years) < 15 else 10\n",
    "        plt.xticks(years[::tick_gap])\n",
    "\n",
    "        # Save the figure as an SVG file\n",
    "        svg_file_path = \"data/inflows_to_processes_{}.svg\".format(process.name)\n",
    "        plt.savefig(svg_file_path, format='svg')\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-17T11:50:37.133254600Z",
     "start_time": "2024-12-17T11:50:37.064230800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\dev\\\\PythonProjects\\\\aiphoria\\\\core\\\\datavisualizer_plotly_post.js'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 54\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m scenario \u001B[38;5;129;01min\u001B[39;00m scenarios:\n\u001B[0;32m     53\u001B[0m     visualizer \u001B[38;5;241m=\u001B[39m DataVisualizer()\n\u001B[1;32m---> 54\u001B[0m     \u001B[43mvisualizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscenario\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualizer_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m     visualizer\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[1;32mC:\\dev\\PythonProjects\\aiphoria\\core\\datavisualizer.py:311\u001B[0m, in \u001B[0;36mDataVisualizer.build\u001B[1;34m(self, scenario, params)\u001B[0m\n\u001B[0;32m    308\u001B[0m filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcore\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatavisualizer_data/datavisualizer_plotly_post.js\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    310\u001B[0m visualizer_js \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 311\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(filename, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fs:\n\u001B[0;32m    312\u001B[0m     visualizer_js \u001B[38;5;241m=\u001B[39m fs\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m    314\u001B[0m visualizer_js \u001B[38;5;241m=\u001B[39m visualizer_js\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{small_node_threshold}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mstr\u001B[39m(small_node_threshold))\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\dev\\\\PythonProjects\\\\aiphoria\\\\core\\\\datavisualizer_plotly_post.js'"
     ]
    }
   ],
   "source": [
    "# *****************************************************\n",
    "# * Step 9: Visualize the flow graph as Sankey graph *\n",
    "# *****************************************************\n",
    "\n",
    "# Color mappings\n",
    "process_transformation_stage_colors = dict()\n",
    "process_transformation_stage_colors[\"Source\"] = \"#7DDA60\"\n",
    "process_transformation_stage_colors[\"First\"] = \"#eb5e34\"\n",
    "process_transformation_stage_colors[\"Second\"] = \"#8c76cf\"\n",
    "process_transformation_stage_colors[\"Third\"] = \"#5BAA11\"\n",
    "process_transformation_stage_colors[\"VAM\"] = \"#3281db\"\n",
    "process_transformation_stage_colors[\"RoW\"] = \"#61b053\"  # Rest of the world\n",
    "process_transformation_stage_colors[\"EoL\"] = \"#EFC3CA\"  # Brown\n",
    "process_transformation_stage_colors[\"by_prod\"] = \"#DFC57B\"  # gold\n",
    "process_transformation_stage_colors[\"Virtual\"] = \"#707070\"\n",
    "process_transformation_stage_colors[\"Other\"] = \"#707070\"\n",
    "\n",
    "# Label overrides\n",
    "virtual_process_graph_labels = dict()\n",
    "virtual_process_graph_labels[\"VP_P2:EU\"] = \"Unreported flow from P2\"\n",
    "virtual_process_graph_labels[\"VP_P3:EU\"] = \"Unreported flow from P3\"\n",
    "\n",
    "# Virtual Process and virtual Flow colors\n",
    "visualizer_params = {\n",
    "    # User can hide processes in Sankey graph that have total inflows less than this value\n",
    "    # This value cannot be changed now in the Sankey graph\n",
    "    \"small_node_threshold\": 5,\n",
    "\n",
    "    # Dictionary to define labels for virtual flows\n",
    "    # If dictionary contains label for the virtual process then that is used,\n",
    "    # otherwise the virtual process ID is used\n",
    "    \"virtual_process_graph_labels\": virtual_process_graph_labels,\n",
    "\n",
    "    # Dictionary to define color of process by the process transformation stage name\n",
    "    # All must be provided as a RGB hex string, prefixed by character '#'\n",
    "    # Usage example: { \"Source\": \"#707070\" }\n",
    "    \"process_transformation_stage_colors\": process_transformation_stage_colors,\n",
    "\n",
    "    # How transparent flows are (0.0 = invisible, 1.0 = fully opaque)\n",
    "    \"flow_alpha\": 0.75,\n",
    "\n",
    "    # Color for virtual process\n",
    "    \"virtual_process_color\": \"rgba(0.3, 0.3, 0.3, 0.6)\",\n",
    "    #\"virtual_process_color\": \"#707070\",\n",
    "\n",
    "    # Color for virtual flows\n",
    "    #\"virtual_flow_color\": \"rgba(0.5, 0.5, 0.5, 0.5)\",\n",
    "    \"virtual_flow_color\": \"#808080\",\n",
    "}\n",
    "\n",
    "# NOTE: Now each scenario is in it's own graph\n",
    "for scenario in scenarios:\n",
    "    visualizer = DataVisualizer()\n",
    "    visualizer.build(scenario, visualizer_params)\n",
    "    visualizer.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-17T11:50:41.703384200Z",
     "start_time": "2024-12-17T11:50:37.095477400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-17T11:50:41.698175800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
